---
title: "STATS 240P - Homework 2"
author: "Scott Murff"
date: "Due November 9, 2015 - 5pm Pacific Time"
output: pdf_document
---

# Problem 1

Prove (3.27) and (3.28)

## Prove 3.27

The results of Sections 2.3.3 and 2.3.4 can be use to obtain the conditional distribution of $\alpha \beta V$ (3.27) (page 78)

$\hat{\boldsymbol{\alpha}} \sim N\Big(\boldsymbol{\alpha},\Big(\frac{1}{n}+\frac{\bar{x}^2}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\Big)\textbf{V}\Big),\hat{\boldsymbol{\beta}} \sim N\Big(\boldsymbol{\beta},\frac{\textbf{V}}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\Big),n\hat{\textbf{V}} \sim W_q\Big(\textbf{V},\textit{n}-2\Big)$ 

To prove the distributions are as stated above I proceed in three steps:

1. Show that $\hat{\boldsymbol{\alpha}}, \hat{\boldsymbol{\beta}}$ are distributed normal with means $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$, respectively. I do this simultaneously using matrix calculations as will be shown.
2. Show that the variances of $\hat{\boldsymbol{\alpha}}$ and $\hat{\boldsymbol{\beta}}$ are as shown above. This is done for $\hat{\boldsymbol{\alpha}}$ and $\hat{\boldsymbol{\beta}}$ separately.
3. Show that $n\hat{\textbf{V}}$ has the disribution shown above.

### Step 1

We represent the model $q$ regression models of $\boldsymbol{y_t}=\boldsymbol{\alpha}+x_t\boldsymbol{\beta}+\boldsymbol{\epsilon_t}, 1\leq t \leq n$, as

$$\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{B} +\boldsymbol{E}$$

Where:

* $\boldsymbol{Y}$ is an $n x q$ matrix of the $\boldsymbol{y_t}$s
* $\boldsymbol{X}$ is an $n x 2$ matrix consisting where the first column is a vector of 1s and the second column is the vector of market returns, $x_t, 1\leq t \leq n$
* $\boldsymbol{B}$ is a $2 x q$ matrix where row 1 is a row vector containing $\alpha_j, j = 1, ...q$ and row two is a row vector containing $\beta_j, j = 1, ...q$
* $\boldsymbol{E}$ is an $n x q$ matrix of the of the $\boldsymbol{\epsilon_t}$s

The OLS estimate of $\boldsymbol{B}$ is $\hat{\boldsymbol{B}}=\boldsymbol{\left(X^T X\right)^{-1}X^TY}$

We can plug in $\boldsymbol{X}\boldsymbol{B} +\boldsymbol{E}$ for $\boldsymbol{Y}$ and then take the expectation as shown below:

$$E\left(\hat{\boldsymbol{B}}\right)=E\left(\boldsymbol{\left(X^T X\right)^{-1}X^T\left(\boldsymbol{X}\boldsymbol{B} +\boldsymbol{E}\right)}\right)$$
$$=E\left(\boldsymbol{\left(X^T X\right)^{-1}X^T\boldsymbol{X}\boldsymbol{B} + \left(X^T X\right)^{-1}X^T\boldsymbol{E}}\right)$$
$$=E\left(\boldsymbol{I\boldsymbol{B} + \left(X^T X\right)^{-1}X^T\boldsymbol{E}}\right)$$
$$=E\left(\boldsymbol{B}\right) + E\left(\boldsymbol{\left(X^T X\right)^{-1}X^TE}\right)$$
$$=\boldsymbol{B} + \boldsymbol{\left(X^T X\right)^{-1}X^T} E\left(\boldsymbol{E}\right)$$
$$=\boldsymbol{B} + \boldsymbol{\left(X^T X\right)^{-1}X^T0}$$
$$=\boldsymbol{B}$$

Recall that $E\left(\boldsymbol{\epsilon_t}\right)=\boldsymbol{0_t}$ so $\boldsymbol{0}$ is an $n x q$ matrix of zeros and cancels out everything except for $\boldsymbol{B}$ and recall that $\boldsymbol{B}$ is a $2 x q$ matrix of the form:


$$ \left(\begin{array}{c}\boldsymbol{\alpha^T}\\\boldsymbol{\beta^T}\end{array}\right)$$

Hence $\hat{\boldsymbol{\alpha}}$ and $\hat{\boldsymbol{\beta}}$ have means $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$, respectively. And since we are operating under the assumption of normally distributed $\boldsymbol{\epsilon_t}$s we can conclude these are the means of a normal distribution.

### Step 2

To derive the variance of $\hat{\boldsymbol{\alpha}}$ and $\hat{\boldsymbol{\beta}}$ we first note that under the Guass-Markov assumptions and normally distributed errors $Cov\left(\bar{\boldsymbol{y}},\bar{x}\hat{\boldsymbol{\beta}}\right)=0$. Also, recall that $\boldsymbol{\alpha}=\bar{\boldsymbol{y}}-\bar{x}\hat{\boldsymbol{\beta}}$. It follows that the variance of $\hat{\boldsymbol{\alpha}}$ can be written as:

$$Var\left(\bar{\boldsymbol{y}}-\bar{x}\hat{\boldsymbol{\beta}}\right)$$

$$= Var\left(\bar{\boldsymbol{y}}\right)+Var\left(\bar{x}\hat{\boldsymbol{\beta}}\right)$$

$$= Var\left(\frac{1}{n}\sum_{t=1}^{n} \boldsymbol{y_t}\right)+Var\left(\frac{\sum_{t=1}^{n}\left(x_t-\bar{x}\right)\boldsymbol{y_t}}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\bar{x}\right)$$

We now work with each term of the two terms above. By the properties of variances and under the assumption of the random varibles being temporally uncorrelated we get:

$$= Var\left(\frac{1}{n}\sum_{t=1}^{n} \boldsymbol{y_t}\right)=\frac{1}{n^2}\sum_{t=1}^{n}Var\left(\boldsymbol{y_t}\right)$$

$$= \frac{1}{n^2}\sum_{t=1}^{n}\boldsymbol{V}$$

$$= \frac{1}{n}\boldsymbol{V}$$

Note that the variance of each $\boldsymbol{y_t}$ is equal to $Var\left(\boldsymbol{\epsilon_t}\right)=\boldsymbol{V}$

We now move to manipulating the second term.

$$Var\left(\frac{\sum_{t=1}^{n}\left(x_t-\bar{x}\right)\boldsymbol{y_t}}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\bar{x}\right)=\sum_{t=1}^{n}Var\left(\boldsymbol{y_t}\frac{\left(x_t-\bar{x}\right)}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\bar{x}\right)$$

$$= \sum_{t=1}^{n}Var\left(\boldsymbol{y_t}\right)\left(\frac{\left(x_t-\bar{x}\right)}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\bar{x}\right)^2$$

$$= \boldsymbol{V}\left(\frac{\sum_{t=1}^{n}\left(x_t-\bar{x}\right)}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\bar{x}\right)^2$$

$$= \frac{1}{n}\boldsymbol{V}\left(\frac{\bar{x}^2}{\frac{1}{n}\sum_{t=1}^{n}\left(x_t-\bar{x}\right)^2}\right)$$

So, the variance of $\hat{\boldsymbol{\alpha}}$ can be written as the following:

$$\frac{1}{n}\boldsymbol{V}+\frac{1}{n}\boldsymbol{V}\left(\frac{\bar{x}^2}{\frac{1}{n}\sum_{t=1}^{n}\left(x_t-\bar{x}\right)^2}\right)$$

$$= \Big(\frac{1}{n}+\frac{\bar{x}^2}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\Big)\textbf{V}$$

Most of the work for deriving the variance of $\hat{\boldsymbol{\beta}}$ was done in manipulating the second term of the variance of $\hat{\boldsymbol{\alpha}}$ as shown above. The only difference is that there is one less $\bar{x}$ term in the variance of $\hat{\boldsymbol{\beta}}$. It follows that:

$$Var\left(\hat{\boldsymbol{\beta}}\right)=Var\left(\frac{\sum_{t=1}^{n}\left(x_t-\bar{x}\right)\boldsymbol{y_t}}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\right)=\sum_{t=1}^{n}Var\left(\boldsymbol{y_t}\frac{\left(x_t-\bar{x}\right)}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\right)$$

$$= \sum_{t=1}^{n}Var\left(\boldsymbol{y_t}\right)\left(\frac{\left(x_t-\bar{x}\right)}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\right)^2$$

$$= \boldsymbol{V}\left(\frac{\sum_{t=1}^{n}\left(x_t-\bar{x}\right)}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\right)^2$$

$$= \boldsymbol{V}\left(\frac{1}{\sum_{t=1}^{n}\left(x_t-\bar{x}\right)^2}\right)$$

$$= \frac{\textbf{V}}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}$$

### Step 3

In order to derive the distribution of $n\hat{\boldsymbol{V}}$ we note that $n\hat{\boldsymbol{V}}=tr\left(\hat{\boldsymbol{E}}\hat{\boldsymbol{E}}^T\right)$ where $\boldsymbol{E}$ is as described in the following formulation:

$$\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{B} +\boldsymbol{E}$$

Where:

* $\boldsymbol{Y}$ is an $n x q$ matrix of the $\boldsymbol{y_t}$s
* $\boldsymbol{X}$ is an $n x 2$ matrix consisting where the first column is a vector of 1s and the second column is the vector of market returns, $x_t, 1\leq t \leq n$
* $\boldsymbol{B}$ is a $2 x q$ matrix where row 1 is a row vector containing $\alpha_j, j = 1, ...q$ and row two is a row vector containing $\beta_j, j = 1, ...q$
* $\boldsymbol{E}$ is an $n x q$ matrix of the of the $\boldsymbol{\epsilon_t}$s

In this case:

$$\hat{\boldsymbol{E}} = \boldsymbol{Y}-\boldsymbol{\left(X^T X\right)^{-1}X^TY}$$

$$=\left(\boldsymbol{I}-\boldsymbol{\left(X^T X\right)^{-1}X^T}\right)\boldsymbol{Y}$$

$$=\boldsymbol{P}\boldsymbol{Y}$$

$\boldsymbol{P}$ is the projection matrix $\boldsymbol{I}-\boldsymbol{\left(X^T X\right)^{-1}X^T}$ and can be decomposed into as $\boldsymbol{P}=\boldsymbol{U}\boldsymbol{U}^T$ where $\boldsymbol{U}$ is an n x (n-2) matrix and each column is othogonal to each column of $\boldsymbol{X}$ and $\boldsymbol{U}$ satisfies $\boldsymbol{U}^T\boldsymbol{U}=\boldsymbol{I}_{n-2}$

It follows that:

$$\hat{\boldsymbol{E}}=tr\left(\boldsymbol{P}\boldsymbol{Y}\boldsymbol{Y}^T\boldsymbol{P}^T\right) = tr\left(\boldsymbol{Y}^T\boldsymbol{P}^T\boldsymbol{P}\boldsymbol{Y}\right)=tr\left(\boldsymbol{Y}^T\boldsymbol{P}\boldsymbol{Y}^T\right)$$

$$=tr\left(\boldsymbol{Y}^T\boldsymbol{U}\boldsymbol{U}^T\boldsymbol{Y}\right) = tr\left(\left(\boldsymbol{U}^T\boldsymbol{Y}\right)^T\left(\boldsymbol{U}^T\boldsymbol{Y}\right)\right))$$

$$=tr\left(\sum_{t=1}^{n-2} \boldsymbol{z}_i\boldsymbol{z}_i^T\right)$$

where for every column $\boldsymbol{u}_i$ of $\boldsymbol{U}$, $\boldsymbol{z}_i^T=\boldsymbol{u}_i^T\boldsymbol{Y}$ is a q-dimensional row vector. Hence $\sum_{t=1}^{n-2} \boldsymbol{z}_i\boldsymbol{z}_i^T$ is a q x q matrix.




## Prove 3.28

In order to show that $\left(\frac{n-q-1}{q}\right)\hat{\boldsymbol{\alpha}}^T\hat{\boldsymbol{V}}^{-1}\hat{\boldsymbol{\alpha}}\left(1+\frac{\bar{x}^2}{n^-1\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\right)^{-1} \sim F_{q,n-q-1}$ we rely on the following theorem:

Let $\boldsymbol{x}$ be a $k x$ 1-vector and $\boldsymbol{A}$ be an $k x k$-matrix. Furthermore, let $\boldsymbol{x}$ and $\boldsymbol{A}$ be independent, $\boldsymbol{x}\sim N\left(\boldsymbol{0},\Omega\right)$, and $\boldsymbol{A}\sim W_j\left(j,\Omega\right)$, where $k\leq j$. Then $\left(\frac{j-k+1}{k}\right)\boldsymbol{x}^T\boldsymbol{A}^{-1}\boldsymbol{x}\sim F_{k,j-k+1}$

Note that as in 3.27 $\hat{\boldsymbol{\alpha}}$ is independent of $\hat{\boldsymbol{V}}$. To untilze the theorem just stated we let:

$$ \boldsymbol{x}=\left(n\left(1+\frac{\bar{x}^2}{n^{-1}\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\right)^{-1}\right)^{1/2}\hat{\boldsymbol{\alpha}},\boldsymbol{A}=n\hat{\boldsymbol{V}},k=q, and j=\left(n-2\right).$$

The only thing that isn't a scalar in $\boldsymbol{x}$ is $\hat{\boldsymbol{\alpha}}$. We showed in the proof of 3.27 above that $\boldsymbol{x}$ and $\hat{\boldsymbol{A}}$ have the distributions shown above. If we plug in value to the theorem shown above we get:

$$\frac{\left(n-2-q+1\right)}{q}\left(\left(n\left(1+\frac{\bar{x}^2}{n^{-1}\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\right)^{-1}\right)^{1/2}\hat{\boldsymbol{\alpha}}\right)^T\left(n\hat{\boldsymbol{V}}\right)^{-1}\left(n\left(1+\frac{\bar{x}^2}{n^{-1}\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\right)^{-1}\right)^{1/2}\hat{\boldsymbol{\alpha}}$$

$$=\frac{\left(n-q-1\right)}{q}\left(1+\frac{\bar{x}^2}{n^{-1}\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\right)^{-1/2}\left(1+\frac{\bar{x}^2}{n^{-1}\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\right)^{-1/2}n^{1/2}n^{-1}n^{1/2}\hat{\boldsymbol{\alpha}}^T\hat{\boldsymbol{V}}^{-1}\hat{\boldsymbol{\alpha}}$$

$$=\frac{\left(n-q-1\right)}{q}\left(1+\frac{\bar{x}^2}{n^{-1}\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\right)^{-1}\hat{\boldsymbol{\alpha}}^T\hat{\boldsymbol{V}}^{-1}\hat{\boldsymbol{\alpha}} \sim F_{q,n-q-1}$$

Hence by using the theorem and 3.27 we have proved what we set out to.



# Problem 2

## (a)

Read Section 9.2.1 of textbook concerning multivariate regression. Use this and Section 1.5.1(linear regression as a minimum-variance lin- ear prediction) to derive the conditional distribution of Y1 given Y2 = y2 in Section 2.3.2. Hint: Review tutorial session of October 21.

## (b) 2.4 (c)

Ex- ercise 2.4(c) (which gives and alternative derivation based on much heavier algebra in Exercise 2.4(a) and (b) that you can assume without carrying out the details)

# Problem 3 - Exercise 2.9

## Part (a)

Show that $E\left(\boldsymbol{\nabla}\log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right) = \boldsymbol{0}$

In order to show this we make use of the fact that if $f_{\boldsymbol{\theta}}$ is a smooth function of $\boldsymbol{\theta}$ then we can exchange the order of expectation and differentiation. First we note that:

$$E\left(\boldsymbol{\nabla}\log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right) = E\Big(\frac{\boldsymbol{\nabla}\ f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}{f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}\Big)$$

This equality follows from the rules of differentiating logs and the chain rule. Next, by the definition of an expectation we write:

$$E\Big(\frac{\boldsymbol{\nabla} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}{\ f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}\Big) = \int \frac{\boldsymbol{\nabla}\ f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}{f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n$$ 

$$=\int \boldsymbol{\nabla} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)$$

We can then switch the order of expectation and differentiation as shown below:

$$=\boldsymbol{\nabla} \int f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n$$
$$=\boldsymbol{\nabla} 1$$
$$=\boldsymbol{0}$$

Note that $\int f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n = 1$ since we are integrating a probability density function. Hence we have shown $E\left(\boldsymbol{\nabla}\log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right) = \boldsymbol{0}$ 

## Part (b)

Show that $E\left(-\boldsymbol{\nabla^2} l\left(\boldsymbol{\theta})\right)\right) = Cov\left(\boldsymbol{\nabla}\log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)$ where $l\left(\boldsymbol{\theta})\right)$ is the log-likelihood function.

We begin by writing:

$$E\left(-\boldsymbol{\nabla^2} l\left(\boldsymbol{\theta})\right)\right) = E\left(-\boldsymbol{\nabla^2} \log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)$$

By taking the first gradient we get:

$$= E\Big(-\boldsymbol{\nabla}\Big(\frac{\boldsymbol{\nabla}\ f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}{f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}\Big)\Big)$$

$$= E\Big(-\frac{\boldsymbol{\nabla^2} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}{f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}+\frac{\boldsymbol{\nabla} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) \left(\boldsymbol{\nabla}f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)^T}{ f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)^2}\Big)$$

$$= \int-\frac{\boldsymbol{\nabla^2} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}{f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n+E\Big(\frac{\boldsymbol{\nabla} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) \left(\boldsymbol{\nabla}f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)^T}{ f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)^2}\Big)$$

$$= \int-\boldsymbol{\nabla^2} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n+E\left(\boldsymbol{\nabla} \log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) \left(\boldsymbol{\nabla}\log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)^T\right)$$

$$= -\boldsymbol{\nabla^2} \Big(\int f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n \Big)+Cov \left(\boldsymbol{\nabla} \log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)$$

$$= -\boldsymbol{\nabla^2} \left(1\right)+Cov \left(\boldsymbol{\nabla} \log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)$$

$$= Cov \left(\boldsymbol{\nabla} \log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)$$

Note that $\int f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n = 1$ since we are integrating a probability density function and $\boldsymbol{\nabla^2} \left(1\right) = 0$. Hence we have shown $E\left(-\boldsymbol{\nabla^2} l\left(\boldsymbol{\theta})\right)\right) = Cov\left(\boldsymbol{\nabla}\log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)$

# Problem 4 - Exercise 3.6

## (a)

CAPM

## (b)

Bootstrap

## (d)

Regression and Hypothesis Test (F-test?)

## (g)

Regression

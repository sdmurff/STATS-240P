---
title: "STATS 240P - Homework 2"
author: "Scott Murff"
date: "Due November 6, 2015"
output: pdf_document
---

# Sections Covered in Lectures

* Friday, October 16 2.3 Multi-variate Normal, 2.1.2 Mean and Covariance Matrix, 3.3 CAPM, 1.4.1 Analysis of Residuals, QQ-Plot
* Wednesday, October 21 TA Session - 2.3, 1.3, 1.4 Regression Diagnostics, Variable Selection, Mulit-Variate Normal
* Friday, October 23 Lecture - 1.5 Stochastic regressors, 1.6.1 Bootstrapping, 1.6.2, 1.6.3, 2.3.1, 2.3.2 Multivariate Normal, 2.3.4 Wishart Distribution

1.4, 1.5, 1.6

2.1.2, 2.3


# Problem 1

Prove (3.27) and (3.28). Hint: Review the derivation of the multivariate t-distribution and the F-distribution and Hotellings T-statistic in the tutorial session of October 21.

## Prove 3.27

The results of Sections 2.3.3 and 2.3.4 can be use to obtain the conditional distribution of $\alpha \beta V$ (3.27) (page 78)

$\hat{\boldsymbol{\alpha}} \sim N\Big(\boldsymbol{\alpha},\Big(\frac{1}{n}+\frac{\bar{x}^2}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\Big)\textbf{V}\Big),\hat{\boldsymbol{\beta}} \sim N\Big(\boldsymbol{\beta},\frac{\textbf{V}}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\Big),n\hat{\textbf{V}} \sim W_q\Big(\textbf{V},\textit{n}-2\Big)$ 

To prove the distributions are as stated above I proceed in three steps:

1. Show that $\hat{\boldsymbol{\alpha}}, \hat{\boldsymbol{\beta}}$ are distributed normal with means $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$, respectively. I do this simultaneously using matrix calculations as will be shown.
2. Show that the variances of $\hat{\boldsymbol{\alpha}}$ and $\hat{\boldsymbol{\beta}}$ are as shown above. This is done for $\hat{\boldsymbol{\alpha}}$ and $\hat{\boldsymbol{\beta}}$ separately.
3. Show that $n\hat{\textbf{V}}$ has the disribution shown above.

### Step 1

We represent the model $q$ regression models of $\boldsymbol{y_t}=\boldsymbol{\alpha}+x_t\boldsymbol{\beta}+\boldsymbol{\epsilon_t}, 1\leq t \leq n$, as

$$\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{B} +\boldsymbol{E}$$

Where:

* $\boldsymbol{Y}$ is an $n x q$ matrix of the $\boldsymbol{y_t}$s
* $\boldsymbol{X}$ is an $n x 2$ matrix consisting where the first column is a vector of 1s and the second column is the vector of market returns, $x_t, 1\leq t \leq n$
* $\boldsymbol{B}$ is a $2 x q$ matrix where row 1 is a row vector containing $\alpha_j, j = 1, ...q$ and row two is a row vector containing $\beta_j, j = 1, ...q$
* $\boldsymbol{E}$ is an $n x q$ matrix of the of the $\boldsymbol{\epsilon_t}$s

The OLS estimate of $\boldsymbol{B}$ is $\hat{\boldsymbol{B}}=\boldsymbol{\left(X^T X\right)^{-1}X^TY}$

We can plug in $\boldsymbol{X}\boldsymbol{B} +\boldsymbol{E}$ for $\boldsymbol{Y}$ and then take the expectation as shown below:

$$E\left(\hat{\boldsymbol{B}}\right)=E\left(\boldsymbol{\left(X^T X\right)^{-1}X^T\left(\boldsymbol{X}\boldsymbol{B} +\boldsymbol{E}\right)}\right)$$
$$=E\left(\boldsymbol{\left(X^T X\right)^{-1}X^T\boldsymbol{X}\boldsymbol{B} + \left(X^T X\right)^{-1}X^T\boldsymbol{E}}\right)$$
$$=E\left(\boldsymbol{I\boldsymbol{B} + \left(X^T X\right)^{-1}X^T\boldsymbol{E}}\right)$$
$$=E\left(\boldsymbol{B}\right) + E\left(\boldsymbol{\left(X^T X\right)^{-1}X^TE}\right)$$
$$=\boldsymbol{B} + \boldsymbol{\left(X^T X\right)^{-1}X^T} E\left(\boldsymbol{E}\right)$$
$$=\boldsymbol{B} + \boldsymbol{\left(X^T X\right)^{-1}X^T0}$$
$$=\boldsymbol{B}$$

Recall that $E\left(\boldsymbol{\epsilon_t}\right)=\boldsymbol{0_t}$ so $\boldsymbol{0}$ is an $n x q$ matrix of zeros and cancels out everything except for $\boldsymbol{B}$ and recall that $\boldsymbol{B}$ is a $2 x q$ matrix of the form:


$$ \left(\begin{array}{c}\boldsymbol{\alpha^T}\\\boldsymbol{\beta^T}\end{array}\right)$$

Hence $\hat{\boldsymbol{\alpha}}$ and $\hat{\boldsymbol{\beta}}$ have means $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$, respectively. And since we are operating under the assumption of normally distributed $\boldsymbol{\epsilon_t}$s we can conclude these are the means of a normal distribution.

### Step 2

To prove the variance of $\hat{\boldsymbol{\alpha}}$ and $\hat{\boldsymbol{\beta}}$ we first note that 

## Prove 3.28

Moreover, arguments similar to those in 2.32 and 2.33 can be used to show that 3.28 is true. (page 78)

# Problem 2

## (a)

Read Section 9.2.1 of textbook concerning multivariate regression. Use this and Section 1.5.1(linear regression as a minimum-variance lin- ear prediction) to derive the conditional distribution of Y1 given Y2 = y2 in Section 2.3.2. Hint: Review tutorial session of October 21.

## (b) 2.4 (c)

Ex- ercise 2.4(c) (which gives and alternative derivation based on much heavier algebra in Exercise 2.4(a) and (b) that you can assume without carrying out the details)

# Problem 3 - Exercise 2.9

## Part (a)

Show that $E\left(\boldsymbol{\nabla}\log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right) = \boldsymbol{0}$

In order to show this we make use of the fact that if $f_{\boldsymbol{\theta}}$ is a smooth function of $\boldsymbol{\theta}$ then we can exchange the order of expectation and differentiation. First we note that:

$$E\left(\boldsymbol{\nabla}\log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right) = E\Big(\frac{\boldsymbol{\nabla}\ f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}{f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}\Big)$$

This equality follows from the rules of differentiating logs and the chain rule. Next, by the definition of an expectation we write:

$$E\Big(\frac{\boldsymbol{\nabla} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}{\ f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}\Big) = \int \frac{\boldsymbol{\nabla}\ f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}{f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n$$ 

$$=\int \boldsymbol{\nabla} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)$$

We can then switch the order of expectation and differentiation as shown below:

$$=\boldsymbol{\nabla} \int f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n$$
$$=\boldsymbol{\nabla} 1$$
$$=\boldsymbol{0}$$

Note that $\int f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n = 1$ since we are integrating a probability density function. Hence we have shown $E\left(\boldsymbol{\nabla}\log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right) = \boldsymbol{0}$ 

## Part (b)

Show that $E\left(-\boldsymbol{\nabla^2} l\left(\boldsymbol{\theta})\right)\right) = Cov\left(\boldsymbol{\nabla}\log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)$ where $l\left(\boldsymbol{\theta})\right)$ is the log-likelihood function.

We begin by writing:

$$E\left(-\boldsymbol{\nabla^2} l\left(\boldsymbol{\theta})\right)\right) = E\left(-\boldsymbol{\nabla^2} \log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)$$

By taking the first gradient we get:

$$= E\Big(-\boldsymbol{\nabla}\Big(\frac{\boldsymbol{\nabla}\ f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}{f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}\Big)\Big)$$

$$= E\Big(-\frac{\boldsymbol{\nabla^2} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}{f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}+\frac{\boldsymbol{\nabla} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) \left(\boldsymbol{\nabla}f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)^T}{ f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)^2}\Big)$$

$$= \int-\frac{\boldsymbol{\nabla^2} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}{f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)}f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n+E\Big(\frac{\boldsymbol{\nabla} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) \left(\boldsymbol{\nabla}f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)^T}{ f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)^2}\Big)$$

$$= \int-\boldsymbol{\nabla^2} f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n+E\left(\boldsymbol{\nabla} \log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) \left(\boldsymbol{\nabla}\log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)^T\right)$$

$$= -\boldsymbol{\nabla^2} \Big(\int f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n \Big)+Cov \left(\boldsymbol{\nabla} \log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)$$

$$= -\boldsymbol{\nabla^2} \left(1\right)+Cov \left(\boldsymbol{\nabla} \log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)$$
$$= Cov \left(\boldsymbol{\nabla} \log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)$$

Note that $\int f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right) dx_1,\ldots,dx_n = 1$ since we are integrating a probability density function and $\boldsymbol{\nabla^2} \left(1\right) = 0$. Hence we have shown $E\left(-\boldsymbol{\nabla^2} l\left(\boldsymbol{\theta})\right)\right) = Cov\left(\boldsymbol{\nabla}\log f_{\boldsymbol{\theta}}\left(X_1,\ldots,X_n\right)\right)$








# Problem 4 - Exercise 3.6

## (a)

CAPM

## (b)

Bootstrap

## (d)

Regression and Hypothesis Test (F-test?)

## (g)

Regression

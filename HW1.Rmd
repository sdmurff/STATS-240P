---
title: "STATS240P-Homework 1"
author: "Scott Murff"
date: " Due October 19, 2015 at 5pm to baijiang@stanford.edu"
output: pdf_document
---

# Exercise 1.2

Using the F-test of a general linear hypothesis, show how to test the following hypotheses on the regression coefficients:

## (a) 
$H_0 : \beta_0 = \beta_2$.

Recall that the full model under consideration is:

$$ y_i = \beta_0 + \beta_1x_{i1}
                 + \beta_2x_{i2}
                 + \beta_3x_{i3}
                 + \epsilon_i, 1\leq i \leq n$$
                 
or in matrix notation:

$$\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}, \,where\, \boldsymbol{Y}\,and\,\boldsymbol{\epsilon} \,are\, \left(n\,x\,1\right), \boldsymbol{X} \,is\, \left(n\,x\,4\right),  \,and\,  \boldsymbol{\beta} \, is \left(\,4\,x\,1\right)$$.
                                               
Since $\beta_0 = \beta_2$ implies $\beta_0 - \beta_2 = 0$, $H_0$ can be represented as $\boldsymbol{\beta}^T\mathbf{c} = 0$ where:

$$\boldsymbol{\beta}^T = \left(\beta_0\,\beta_1\,\beta_2\,\beta_3\right)$$ 

and 

$$\mathbf{c} = \left(\begin{array}{c}1\\0\\-1\\0\end{array}\right)$$

This constraint results in the following restricted model:

$$ y_i = \beta_0\left(1+x_{i2}\right) + \beta_1x_{i1}
                                      + \beta_3x_{i3}
                                      + \epsilon_i$$
or in matrix notation:

$$\boldsymbol{Y} = \boldsymbol{\tilde{X}\tilde{\beta}} + \boldsymbol{\epsilon}, \,where\, \boldsymbol{Y}\,and\,\boldsymbol{\epsilon} \,are\, \left(n\,x\,1\right), \boldsymbol{\tilde{X}} \,is\, \left(n\,x\,3\right),  \,and\,  \boldsymbol{\tilde{\beta}} \, is \left(\,3\,x\,1\right)$$.

Once the full and restricted models are defined, performing an F-test then proceeds in the following steps:

1. Estimate $\boldsymbol{\beta}$ in the full model using the relationship $\boldsymbol{\hat{\beta}} = \left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{Y}$. It follows that $\hat{\boldsymbol{Y}} = \boldsymbol{X}\left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{Y}$. Note that $\mathop{\mathbb{E\left(\epsilon\right)}} = \boldsymbol{0}$ 
2. Compute the RSS of the full model by $||\boldsymbol{Y}-\hat{\boldsymbol{Y}}||^2$
3. Estimate $\boldsymbol{\tilde{\beta}}$ in the full model using the relationship $\boldsymbol{\hat{\tilde{\beta}}} = \left(\boldsymbol{\tilde{X}}^T\boldsymbol{\tilde{X}}\right)^{-1}\boldsymbol{\tilde{X}}^T\boldsymbol{Y}$. It follows that $\hat{\boldsymbol{Y_0}} = \boldsymbol{\tilde{X}}\left(\boldsymbol{\tilde{X}}^T\boldsymbol{\tilde{X}}\right)^{-1}\boldsymbol{\tilde{X}}^T\boldsymbol{Y}$. Note that $\mathop{\mathbb{E\left(\epsilon\right)}} = \boldsymbol{0}$ 
4. Compute the RSS of the restricted model by $||\boldsymbol{Y}-\hat{\boldsymbol{Y_0}}||^2$

The RSS of the full and the RSS of restricted model can be viewed geometrically as in the picture below (image taken from course textbook)

![](H:/Stanford/STATS240P/Ftest.PNG)

As can be seen in the picture, $||\boldsymbol{Y}-\hat{\boldsymbol{Y_0}}||^2 = ||\boldsymbol{Y}-\hat{\boldsymbol{Y}}||^2 + ||\hat{\boldsymbol{Y}}-\hat{\boldsymbol{Y_0}}||^2$ 

This relationship holds by Pythagoras' theorem for projections and the fact that $||\boldsymbol{Y}-\hat{\boldsymbol{Y}}||^2$ and $||\hat{\boldsymbol{Y}}-\hat{\boldsymbol{Y_0}}||^2$ are orthogonal.

We want a test that will tell us if the term $||\hat{\boldsymbol{Y}}-\hat{\boldsymbol{Y_0}}||^2$ is sufficiently large to conclude that the full model is superior to the restricted model. So we proceed to step 5.

5. Construct an F-Statistic as follows: $F = \dfrac{\frac{||\hat{\boldsymbol{Y}}-\hat{\boldsymbol{Y_0}}||^2}{p-r}}{\frac{||\boldsymbol{Y}-\hat{\boldsymbol{Y}}||^2}{n-p}}$, where the degrees of freedom in the denominator are n-4, and the degrees of freedom in the numerator are 4-3. Under the null $F \overset{H_0}{\sim} F(1,n-4)$. If the value of $F$ is larger than the value of the quantile $F_{1,n-4;\alpha}$ then we reject the null and conclude that the full model is superior. Otherwise we fail to reject the null and use the restricted model.

## (b) 
$H_0 : \beta_1 + \beta_2 = 3\beta_3$.

To test this hypothesis we follow the same logic and procedure outlined in a) above with the following adjustments:

1. In this case $H_0$ can be represented as $\beta_1 + \beta_2 - 3\beta_3 = 0$ or $\boldsymbol{\beta}^T\mathbf{c} = 0$ where:

$$\boldsymbol{\beta}^T = \left(\beta_0\,\beta_1\,\beta_2\,\beta_3\right)$$ 

and 

$$\mathbf{c} = \left(\begin{array}{c}0\\1\\1\\-3\end{array}\right)$$

A convenient way to represent the constraint to faciliate esimating the reduced model is to write $\beta_1 = 3\beta_3 -\beta_2$. We can then plug $\beta_1$ back into the full model as follows:
$$ y_i = \beta_0 + \left(3\beta_3-\beta_2\right)x_{i1}
                 + \beta_2x_{i2}
                 + \beta_3x_{i3}
                 + \epsilon_i, 1\leq i \leq n$$
                 
After algebraic manipulation, this reduces to:

$$ y_i = \beta_0 + \beta_2\left(x_{i2}-x_{i1}\right)
                 + \beta_3\left(3x_{i1}-x_{i3}\right)
                 + \epsilon_i, 1\leq i \leq n$$
or in matrix notation:

$$\boldsymbol{Y} = \boldsymbol{\tilde{X}\tilde{\beta}} + \boldsymbol{\epsilon}, \,where\, \boldsymbol{Y}\,and\,\boldsymbol{\epsilon} \,are\, \left(n\,x\,1\right), \boldsymbol{\tilde{X}} \,is\, \left(n\,x\,3\right),  \,and\,  \boldsymbol{\tilde{\beta}} \, is \left(\,3\,x\,1\right)$$.

After obtaining $\boldsymbol{\hat{\tilde{\beta}}}$ from the reduced model and $\boldsymbol{\hat{\beta}}$ from the full model we can compute $\boldsymbol{\hat{Y}} = \boldsymbol{X}\boldsymbol{\hat{\beta}}$ and $\boldsymbol{\hat{Y_0}} = \boldsymbol{\tilde{X}}\boldsymbol{\hat{\tilde{\beta}}}$ and then compute the F statistic $F = \dfrac{\frac{||\hat{\boldsymbol{Y}}-\hat{\boldsymbol{Y_0}}||^2}{4-3}}{\frac{||\boldsymbol{Y}-\hat{\boldsymbol{Y}}||^2}{n-4}}$. Under the null $F\overset{H_0}{\sim} F(1,n-4)$. If the value of $F$ is larger than the value of the quantile $F_{1,n-4;\alpha}$ then we reject the null and conclude that the full model is superior. Otherwise we fail to reject the null and use the restricted model.

# Exercise 1.6

## (a)

```{r echo=FALSE, warning=FALSE}
library(knitr)
read_chunk('H:/Stanford/STATS240P/Homework 1.R')
```

```{r HW1point6, echo=FALSE, results='hide', eval=TRUE, warning=FALSE, message=FALSE}
```

```{r a1_6_1, fig.height=4, fig.width=6}
```

An initial glance at the plot suggests that the i.i.d. assumption for Pfizer returns is a reasonable one. After adding a horizontal line at 0 and a verical line at March 8, 1999 there does appear to be a slight downward shift of the mean of the returns after this date. It is not easy to see visually however from a scatter plot. A difference is more clear by looking at a box plot as shown below:

```{r a1_6_2, fig.height=4, fig.width=6}
```

## (b)
A regression summary is shown below:
```{r b1_6_1}
```

95 % confidence Intervals for the paramters are shown below:
```{r b1_6_2}
```

## (c)
Test $H_0 : \mu_1 = \mu_2$.

Testing $H_0$ can achieved by performing an F-test with the full model being $r_t = \mu_1\boldsymbol{1}_{\{t<t_0\}} + \mu_2\boldsymbol{1}_{\{t \geq t_0\}} + \epsilon_t$ and the reduced model being $r_t = \mu_1\boldsymbol{1}$. We fit the reduced model below and then carry out an F-test using anova commnand in R as shown below:

```{r c1_6_1}
```

As can be seen in the output above, the F-Statistic under the null is 6.009 resulting in a p-value of 0.01436. This provides sufficient evidence to reject $H_0 : \mu_1 = \mu_2$ at the 5% significance level and conclude that there is reasonably strong evidence $\mu_1$ and $\mu_2$ are different. 


# Exercise 2.2

## (a)

```{r HW2point2, echo=TRUE, results='hide', eval=TRUE, warning=FALSE, message=FALSE}
```

```{r a2_2_1}
```

My manual version of PCA with the covariance matrix matches (with minor rounding error) the output from princomp. The results show that nearly 93 percent of the total variance can be captured in the first principal component. Over 99 percent of the variance can be captured in the first two principal components. The plot from the screenplot function shows a plot of the variances attributable to each component. These results show that it would be reasonable to use just the first two principal compenents for analysis.

## (b)

```{r b2_2_1}
```

My manual version of PCA with the correlation matrix matches (with minor rounding error) the output from princomp. The results show that nearly 89 percent of the total variance can be captured in the first principal component. Over 99 percent of the variance can be captured in the first two principal components. The plot from the screenplot function shows a plot of the variances attributable to each component. These results show that it would be reasonable to use just the first two principal compenents for analysis.

## (c)

This question asks for a comparison of PCA results from Section 2.2.3 from the textbook, which uses daily swap data, to the PCA results using monthly data. There seems to be an inconsistency however, since the results from 2.2.3 are based on differenced daily data, while the results from parts (a) and (b) are not based on differenced data. I proceed to show two comparisons:

1. An seemingly apples to oranges comparison with daily differenced data and monthly non-differenced data.
2. An apples to apples comparison where I difference the monthly data as well and then compare those results.

```{r c2_2_1}
```


```{r c2_2_2}
```

In the apples to apples comparison, daily vs monthly data seems to make little difference. In the case of using the covariance matrix, the 1st component of the daily data accounts for slightly more variance than the 1st component of the monthly data,  0.9268570 vs 0.9176055, respectively. However, the 2nd component of the daily data accounts for slightly less variance than the 2nd compoenent of the monthly data, 0.0527213 vs  0.07234153, so that the cumulative variance is actually greater for the monthly data and stays slightly higher for all the rest of the components. The differences may not be meaningful however. In the case of using the correlation matrix, the story is same. However, the differences do not appear to be large and therefore it may not make a big difference whether daily or monthly samples are used in analysis, especially after the first two compenents are selected.

# Exercise 2.3

```{r HW2point3, echo=TRUE, results='hide', eval=TRUE, warning=FALSE, message=FALSE}
```

## (a)

```{r c2_3_1}
```

This PCA analysis shows that the first principal component accounts for roughly 47 percent of the variance. Subsequent components each account for a decreasing but still significant portion of the variance. You may need to use many of the components to avoid losing significant information.

## (b)

```{r c2_3_2}
```

This PCA analysis shows that the first principal component accounts for roughly 48 percent of the variance, similar to when using the covariance matrix. Subsequent components each account for a decreasing but still significant portion of the variance. You may need to use many of the components to avoid losing significant information.


---
title: "STATS 240P - Homework 2"
author: "Scott Murff"
date: "Due November 6, 2015"
output: pdf_document
---

# Sections Covered in Lectures

* Friday, October 16 2.3 Multi-variate Normal, 2.1.2 Mean and Covariance Matrix, 3.3 CAPM, 1.4.1 Analysis of Residuals, QQ-Plot
* Wednesday, October 21 TA Session - 2.3, 1.3, 1.4 Regression Diagnostics, Variable Selection, Mulit-Variate Normal
* Friday, October 23 Lecture - 1.5 Stochastic regressors, 1.6.1 Bootstrapping, 1.6.2, 1.6.3, 2.3.1, 2.3.2 Multivariate Normal, 2.3.4 Wishart Distribution

1.4, 1.5, 1.6

2.1.2, 2.3


# Problem 1

Prove (3.27) and (3.28). Hint: Review the derivation of the multivariate t-distribution and the F-distribution and Hotellings T-statistic in the tutorial session of October 21.

## Prove 3.27

The results of Sections 2.3.3 and 2.3.4 can be use to obtain the conditional distribution of $\alpha \beta V$ (3.27) (page 78)

$\hat{\boldsymbol{\alpha}} \sim N\Big(\boldsymbol{\alpha},\Big(\frac{1}{n}+\frac{\bar{x}^2}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\Big)\textbf{V}\Big),\hat{\boldsymbol{\beta}} \sim N\Big(\boldsymbol{\beta},\frac{\textbf{V}}{\sum_{t=1}^{n} \left(x_t-\bar{x}\right)^2}\Big),n\hat{\textbf{V}} \sim W_q\Big(\textbf{V},\textit{n}-2\Big)$ 

To prove the distributions are as stated above I proceed in three steps:

1. Show that $\hat{\boldsymbol{\alpha}}, \hat{\boldsymbol{\beta}}$ are distributed normal with means $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$, respectively. I do this simultaneously using matrix calculations as will be shown.
2. Show that the variances of $\hat{\boldsymbol{\alpha}}$ and $\hat{\boldsymbol{\beta}}$ are as shown above. This is done for $\hat{\boldsymbol{\alpha}}$ and $\hat{\boldsymbol{\beta}}$ separately.
3. Show that $n\hat{\textbf{V}}$ has the disribution shown above.

1.

We represent the model $q$ regression models of $\boldsymbol{y_t}=\boldsymbol{\alpha}+x_t\boldsymbol{\beta}+\boldsymbol{\epsilon_t}, 1\leq t \leq n$, as

$$\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{B} +\boldsymbol{E}$$

Where:

* $\boldsymbol{Y}$ is an $n x q$ matrix of the $\boldsymbol{y_t}$s
* $\boldsymbol{X}$ is an $n x 2$ matrix consisting where the first column is a vector of 1s and the second column is the vector of market returns, $x_t, 1\leq t \leq n$
* $\boldsymbol{B}$ is a $2 x q$ matrix where row 1 is a row vector containing $\alpha_j, j = 1, ...q$ and row two is a row vector containing $\beta_j, j = 1, ...q$
* $\boldsymbol{E}$ is an $n x q$ matrix of the of the $\boldsymbol{\epsilon_t}$s

The OLS estimate of $\boldsymbol{B}$ is $\hat{\boldsymbol{B}}=\boldsymbol{\left(X^T X\right)^{-1}X^TY}$

We can plug in $\boldsymbol{X}\boldsymbol{B} +\boldsymbol{E}$ for $\boldsymbol{Y}$ and then take the expectation as shown below:

$$E\left(\hat{\boldsymbol{B}}\right)=E\left(\boldsymbol{\left(X^T X\right)^{-1}X^T\left(\boldsymbol{X}\boldsymbol{B} +\boldsymbol{E}\right)}\right)$$
$$=E\left(\boldsymbol{\left(X^T X\right)^{-1}X^T\boldsymbol{X}\boldsymbol{B} + \left(X^T X\right)^{-1}X^T\boldsymbol{E}}\right)$$
$$=E\left(\boldsymbol{I\boldsymbol{B} + \left(X^T X\right)^{-1}X^T\boldsymbol{E}}\right)$$
$$=E\left(\boldsymbol{B}\right) + E\left(\boldsymbol{\left(X^T X\right)^{-1}X^TE}\right)$$
$$=\boldsymbol{B} + \boldsymbol{\left(X^T X\right)^{-1}X^T} E\left(\boldsymbol{E}\right)$$
$$=\boldsymbol{B} + \boldsymbol{\left(X^T X\right)^{-1}X^T0}$$
$$=\boldsymbol{B}$$

Recall that $E\left(\boldsymbol{\epsilon_t}\right)=\boldsymbol{0_t}$ so $\boldsymbol{0}$ is an $n x q$ matrix of zeros and cancels out everything except for $\boldsymbol{B}$ and recall that $\boldsymbol{B}$ is:


$$ \left(\begin{array}{c}\boldsymbol{\alpha^T}\\\boldsymbol{\beta^T}\end{array}\right)$$

## Prove 3.28

Moreover, arguments similar to those in 2.32 and 2.33 can be used to show that 3.28 is true. (page 78)

# Bullet Point 2

## (a)

Read Section 9.2.1 of textbook concerning multivariate regression. Use this and Section 1.5.1(linear regression as a minimum-variance lin- ear prediction) to derive the conditional distribution of Y1 given Y2 = y2 in Section 2.3.2. Hint: Review tutorial session of October 21.

## (b) 2.4 (c)

Ex- ercise 2.4(c) (which gives and alternative derivation based on much heavier algebra in Exercise 2.4(a) and (b) that you can assume without carrying out the details)

# Bullet Point 3 - Exercise 2.9

# Bullet Point 4 - Exercise 3.6

## (a)

CAPM

## (b)

Bootstrap

## (d)

Regression and Hypothesis Test (F-test?)

## (g)

Regression
